ind
don[ind,]
class(don)
ind
don[ind,]
ind <- createDataPartition(don$Y, 0.7, list=F)
ind <- createDataPartition(don$Y, 0.7, list=T)
donA <- don[ind,]
ind <- createDataPartition(don, 0.7, list=T)
ind <- createDataPartition(don, 0.7, list=F)
ind <- createDataPartition(don[,"Y"], 0.7, list=F)
library(caret)
ind <- createDataPartition(don[,"Y"], 0.7, list=F)
ind <- createDataPartition(don[,"Y"], 0.7, list=T)
ind <- as.data.frame(createDataPartition(don[,"Y"], 0.7, list=T))
ind
donA <- don[ind,]
ind
ind <- as.data.frame(createDataPartition(don[,"Y"], 0.7, list=T))[1]
ind
ind <- as.data.frame(createDataPartition(don[,"Y"], 0.7, list=T))[1]
ind <- as.data.frame(createDataPartition(don[,"Y"], 0.7, list=T))[,1]
ind
don[ind,]
ind <- createDataPartition(don[,"Y"], 0.7, list=T)[,1]
ind <- createDataPartition(don[,"Y"], 0.7, list=F)[,1]
ind <- as.data.frame(createDataPartition(don[,"Y"], 0.7)[,1])
levels(don$Y)
don$Y
on <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
summary(don)
levels(don$Y)
createDataPartition(don[,"Y"], 0.7, list=F)
don[,"Y"]
createDataPartition(don[,"Y"], p=0.7, list=F)
ind <- createDataPartition(don[,"Y"], p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
table(don$Y)
prop.table(table(donA$Y))
prop.table(table(don$Y))
mod <- train(Y~., data=donA, methode="glm",trControl=fitControl)
library(caret)
ind <- createDataPartition(don[,"Y"], p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
prop.table(table(don$Y))
prop.table(table(donA$Y))
fitControl <- trainControl(method="none")
mod <- train(Y~., data=donA, methode="glm",trControl=fitControl)
mod
mod <- train(Y~., data=donA, methode="glm",trControl=fitControl)
mod
mod <- train(Y~., data=donA, method="glm",trControl=fitControl)
mod
mod$finalModel
res=data.frame(don$Y,"log"=0)
res <- predict(mod,newdata = donT)
res=data.frame(don$Y,"log"=0)
res[,"log"] <- predict(mod,newdata = donT)
predict(mod,newdata = donT)
res[,"log"]
res[,"log"] <- predict(mod,newdata = donT)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
library(caret)
res=data.frame(don$Y,"log"=0)
ind <- createDataPartition(don[,"Y"], p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
prop.table(table(don$Y))
prop.table(table(donA$Y))
fitControl <- trainControl(method="none")
mod <- train(Y~., data=donA, method="glm",trControl=fitControl)
res[,"log"] <- predict(mod,newdata = donT)
dim(don)
dim(donA)
dim(donT)
res=data.frame(donT$Y,"log"=0)
res[,"log"] <- predict(mod,newdata = donT)
res[,"log"]
confusionMatrix(data = res[,"log"], reference = res[,"Y"], positive = "Yes" )
confusionMatrix(data = res[,"log"], reference = res[,"Y"], positive = "Yes")
don
res=data.frame("Y"=donT$Y,"log"=0)
confusionMatrix(data = res[,"log"], reference = res[,"Y"], positive = "Yes")
confusionMatrix(data = res[,"log"], reference = res[,"Y"], positive = "Yes")
library(caret)
res=data.frame("Y"=donT$Y,"log"=0)
ind <- createDataPartition(don[,"Y"], p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
prop.table(table(don$Y))
prop.table(table(donA$Y))
fitControl <- trainControl(method="none")
mod <- train(Y~., data=donA, method="glm",trControl=fitControl)
res[,"log"] <- predict(mod,newdata = donT)
res[,"log"]
res[,"Y"]
res[,"log"]
confusionMatrix(data = res[,"log"], reference = res[,"Y"], positive = "Yes")
res[,"log"] <- predict(mod,newdata = donT,type = "response")
res[,"log"] <- predict(mod,newdata = donT,type = "raw")
res[,"log"]
res[,"log"] <- predict(mod,newdata = donT,type = "prob")
res[,"log"]
quantile(res[,"log"])
lift(donT$Y~res[,"log"],donT, class="Yes")
plot(lift(donT$Y~res[,"log"],donT, class="Yes"))
plot(lift(donT$Y~res[,"log"],donT, class="No"))
lift_obj <- lift(donT$Y~res[,"log"],donT, class="No")
plot(lift_obj)
lift_obj <- lift(donT$Y~res[,"log"],donT, class="Yes")
plot(lift_obj)
library(caret)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
res=data.frame("Y"=donT$Y,"log"=0)
ind <- createDataPartition(don[,"Y"], p=0.75, list=F)
ind
donA <- don[ind,]
donT <- don[-ind,]
-ind
don[-ind,]
-ind
prop.table(table(don$Y))
prop.table(table(donA$Y))
fitControl <- trainControl(method="none")
mod <- train(Y~., data=donA, method="glm",trControl=fitControl)
res[,"log"] <- predict(mod,newdata = donT,type = "raw")
rm(list=ls())
library(caret)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
res=data.frame("Y"=donT$Y,"log"=0)
ind <- createDataPartition(don[,"Y"], p=0.75, list=F)
donA <- don[ind,]
donT <- don[-ind,]
prop.table(table(don$Y))
prop.table(table(donA$Y))
res=data.frame("Y"=donT$Y,"log"=0)
####################################
library(caret)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
ind <- createDataPartition(don[,"Y"], p=0.75, list=F)
donA <- don[ind,]
donT <- don[-ind,]
prop.table(table(don$Y))
prop.table(table(donA$Y))
res=data.frame("Y"=donT$Y,"log"=0)
fitControl <- trainControl(method="none")
mod <- train(Y~., data=donA, method="glm",trControl=fitControl)
res[,"log"] <- predict(mod,newdata = donT,type = "raw")
confusionMatrix(data = res[,"log"], reference = res[,"Y"], positive = "Yes")
res[,"log"] <- predict(mod,newdata = donT,type = "prob")
predict(mod,newdata = donT,type = "prob")
res[,"log"] <- predict(mod,newdata = donT,type = "prob")[,"Yes"]
res[,"log"]
quantile(res[,"log"])
lift_obj <- lift(donT$Y~res[,"log"],donT, class="Yes")
plot(lift_obj)
library(pRoc)
library(pROC)
roc(res[,"log"],res[,"Y"])
res[,"log"]
res[,"Y"]
res[,"log"] <- predict(mod,newdata = donT,type = "raw")
roc(res[,"log"],res[,"Y"])
res[,"log"]
roc(res[,"Y"],res[,"log"])
plot(roc(res[,"Y"],res[,"log"]))
res[,"log"] <- predict(mod,newdata = donT,type = "prob")[,"Yes"]
plot(roc(res[,"Y"]=="Yes",res[,"log"]))
res[,"Y"]=="Yes"
roc(res[,"Y"]=="Yes",res[,"log"])
res[,"log"]
roc_obj <- roc(res[,"Y"]=="Yes",res[,"log"])
plot(1-roc_obj$specificities,roc_objet$sensitivities, type="l")
roc_obj <- roc(res[,"Y"]=="Yes",res[,"log"])
plot(1-roc_obj$specificities,roc_objet$sensitivities, type="l")
plot(1-roc_obj$specificities,roc_obj$sensitivities, type="l")
abline(0,1)
roc_obj$AUC
roc_obj$auc
fitControl <- trainControl(method="cv", number=10)
mod <- train(Y~., data=donA, method="glm",trControl=fitControl)
plot(roc_obj)
varImp(mod)
#Méthode de selection
m_lrs <- train(Y~., data=donA, method="glmStepAIC", trControl=trainControl(none))
#Méthode de selection
m_lrs <- train(Y~., data=donA, method="glmStepAIC", trControl=trainControl("none"))
library(caret)
library(pROC)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
ind <- createDataPartition(don$Y,p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
res <- data.frame("Y"=donT$Y, "log"=0, "lift"=0)
# Methode d'apprentissage sans CV avec glm
fitControl<- trainControl(method = "none")
mod <- train(Y~.,donA ,method = "glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type = "raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
# Methode d'apprentissage avec cv avec glm
# fitControl <- trainControl(method = "cv", number=10)
# mod <- train(Y~.,donA, method="glm", trControl=fitControl)
# res[,"log"] <- predict(mod, donT,type="raw")
# mat <- confusionMatrix(res[,"Y"], res[,"log"])
# choix de variable
# fitControl <- trainControl(method="none")
# mod <- train(Y~., donA, method="glmStepAIC",trControl=fitControl)
# varImp(mod)
# lift
res[,"lift"] <- predict(mod, donT, type="prob")["Yes"]
plot(lift(res[,"Y"]~res[,"lift"], data=donT, class = "Yes"))
# pRoc
library(pROC)
plot(roc(res[,"Y"],res[,"log"]))
res[,"Y"]
roc(res[,"Y"],res[,"log"])
res[,"log"]
roc(res[,"Y"],res[,"lift"])
plot(roc(res[,"Y"],res[,"lift"])
plot(roc(res[,"Y"],res[,"lift"]))
roc(res[,"Y"],res[,"lift"])
roc(res[,"Y"]=="yes",res[,"lift"])
roc(res[,"Y"]=="Yes",res[,"lift"])
roc(res[,"Y"],res[,"lift"])
roc(res[,"Y"]=="Yes",res[,"lift"])
rocobj <- roc(res[,"Y"]=="Yes",res[,"lift"])
plot(1-rocobj$specificities, rocobj$sensitivities, type="l")
plot(rocobj$specificities, rocobj$sensitivities, type="l")
plot(rocobj$specificities,1- rocobj$sensitivities, type="l")
plot(rocobj$specificities, rocobj$sensitivities, type="l")
plot(1-rocobj$specificities, rocobj$sensitivities, type="l")
rocobj <- roc(res[,"Y"],res[,"lift"])
plot(1-rocobj$specificities, rocobj$sensitivities, type="l")
abline(0,1)
res$resample
mod$resample
mod$resample
####################################
#Avec Caret
####################################
library(caret)
library(pROC)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
ind <- createDataPartition(don$Y,p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
res <- data.frame("Y"=donT$Y, "log"=0, "lift"=0)
# Methode d'apprentissage sans CV avec glm
fitControl<- trainControl(method = "none")
mod <- train(Y~.,donA ,method = "glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type = "raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
# Methode d'apprentissage avec cv avec glm
# fitControl <- trainControl(method = "cv", number=10)
# mod <- train(Y~.,donA, method="glm", trControl=fitControl)
# res[,"log"] <- predict(mod, donT,type="raw")
# mat <- confusionMatrix(res[,"Y"], res[,"log"])
# choix de variable
# fitControl <- trainControl(method="none")
# mod <- train(Y~., donA, method="glmStepAIC",trControl=fitControl)
# varImp(mod)
# lift
res[,"lift"] <- predict(mod, donT, type="prob")["Yes"]
plot(lift(res[,"Y"]~res[,"lift"], data=donT, class = "Yes"))
# pRoc
library(pROC)
plot(roc(res[,"Y"],res[,"lift"]))
rocobj <- roc(res[,"Y"],res[,"lift"])
rocobj <- roc(res[,"Y"]=="Yes",res[,"lift"])
plot(1-rocobj$specificities, rocobj$sensitivities, type="l")
abline(0,1)
mod$resample
fitControl <- trainControl(method = "cv", number=4)
mod <- train(Y~.,donA, method="glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type="raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
mod$resample
plot(rocobj)
varImp(mod)
mod$finalModel
length(mod$finalModel$coefficients)-1
dim(don)
mod <- train(Y~, donA, method = "svm", trControl=fitControl)
# Methode d'apprentissage avec SVM
fitControl <- trainControl(method = "none")
mod <- train(Y~., donA, method = "svm", trControl=fitControl)
mod <- train(Y~., donA, method = "svmLinear", trControl=fitControl)
res <- data.frame("Y"=donT$Y, "log"=0, "lift"=0, "svm"=0)
predict(mod,donT)
predict(mod,donT, type="raw")
predict(mod,donT, type="prob")
predict(mod,donT, type="prob")["Yes"]
res[,"svm"] <- predict(mod,donT)
res[,"svm"]
mat <- confusionMatrix(res[,"Y"],res[,"svm"])
mat
mat <- confusionMatrix(res[,"Y"],res[,"svm"], positive="Yes")
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.1, 0.5, 1, 10)))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=c(0.1, 0.5, 1, 10))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(c(0.1, 0.5, 1, 10)))
data.frame(C=c(0.1, 0.5, 1, 10))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.1, 0.5, 1, 10)))
data.frame(C=c(0.1, 0.5, 1, 10))
donA
train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.1, 0.5, 1, 10)))
c(0.1, 0.5, 1, 10)
C=c(0.1, 0.5, 1, 10)
C
train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.1, 0.5, 1, 10)))
####################################
#Avec Caret
####################################
library(caret)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.1, 0.5, 1, 10)))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(c(0.1, 0.5, 1, 10)))
data.frame(c(0.1, 0.5, 1, 10))
data.frame(0.1, 0.5, 1, 10)
train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(0.1, 0.5, 1, 10))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=c(0.1, 0.5, 1, 10))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=0.5)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=matrix(0.5,0.3))
matrix(0.5,0.3)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.5,1.5)))
fitControl <- trainControl(method = "cv",number=4)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.5,1.5)))
# Recherche du parametre cout optimum pour le SVM
fitControl <- trainControl(method = "cv",number=4)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=c(0.5,1.5)))
data.frame(C=c(0.5,1.5)
data.frame(C=c(0.5,1.5))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=0.5))
data.frame(C=0.5)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=data.frame(C=0.5))
devtools::install_github("caret")
expand.grid(mtry = 100)
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=expand.grid(mtry=0.5))
mod <- train(Y~., data=donA, methode="svmLinear",trControl=fitControl, tuneGrid=expand.grid(mtry=c(10,20)))
dim(donA)
mod
rfe(Y~., donA, sizes=c(5,10,15,20), rfeControl = fitControl)
rfe(donA[,-89], donA$Y, sizes=c(5,10,15,20), rfeControl = fitControl)
# Recursive feature elimination
fitControl <- rfeControl(functions = lrFuncs, method="cv", number = 4)
rfe(donA[,-89], donA$Y, sizes=c(5,10,15,20), rfeControl = fitControl)
donA$Y
donA[,-89]
donA[-89]
rfe(donA[,-89], donA$Y, sizes=c(5,10,15,20), rfeControl = fitControl)
rfe(Y~., donA, sizes=c(5,10,15,20), rfeControl = fitControl)
c(5,10,15,20)
dim(don)
donA[,-89]
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), rfeControl = fitControl)
# Recursive feature elimination
fitControl <- rfeControl(functions = lrFuncs, number = 4)
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), method="cv", rfeControl = fitControl)
donA[,89
donA[,89]
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), method="cv", rfeControl = fitControl)
# Recursive feature elimination
fitControl <- rfeControl(functions = lrFuncs, method="cv", number = 4)
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), rfeControl = fitControl)
# Recursive feature elimination
fitControl <- rfeControl(functions = lrFuncs)
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), rfeControl = fitControl)
# Recursive feature elimination
fitControl <- rfeControl(functions = lrFuncs, method="cv", number=4)
donA[,-89]
donA[,89]
as.data.frame(donA[,89])
rfe(donA[,-89], as.data.frame(donA[,89]), sizes=c(5,10,15,20), rfeControl = fitControl)
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), rfeControl = fitControl)
sizes=c(5,10,15,20)
sizes
rfe(donA[,-89], donA[,89], sizes=data.frame(c(5,10,15,20)), rfeControl = fitControl)
rfe(donA[,-89], donA[,89], sizes=data.frame(C=c(5,10,15,20)), rfeControl = fitControl)
# Recursive feature elimination
fitControl <- rfeControl(functions = caretFuncs, method="cv", number=4)
rfe(donA[,-89], donA[,89], sizes=c(5,10,15,20), rfeControl = fitControl)
evaluation <- function(approche){
mod <- train(Y~., data= donA, trControl=trainControl(method = "cv", number = 5), method=approche)
pred <- predict(mod, newdata = donT)
mat <- confusionMatrix(pred,donT$Y, positive="yes")
return(c(max(mod$results[,"Accuracy"]),pref$results[,"Accuracy"]))
}
methodes <- c("glm","lda","svmLinear")
perfs <- sapply(methodes,evaluation)
methodes <- c("glm")
perfs <- sapply(methodes,evaluation)
rm(list=ls())
library(caret)
library(pROC)
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
ind <- createDataPartition(don$Y,p=0.7, list=F)
donA <- don[ind,]
donT <- don[-ind,]
# Comparaison methode
evaluation <- function(approche){
mod <- train(Y~., data= donA, trControl=trainControl(method = "cv", number = 5), method=approche)
pred <- predict(mod, newdata = donT)
mat <- confusionMatrix(pred,donT$Y, positive="yes")
return(c(max(mod$results[,"Accuracy"]),pref$results[,"Accuracy"]))
}
methodes <- c("glm")
perfs <- sapply(methodes,evaluation)
perfs <- apply(methodes,1,evaluation)
methodes <- c("glm","svm")
perfs <- apply(methodes,1,evaluation)
c("glm","svm")
evaluation("glm")
mat <- confusionMatrix(pred,donT$Y, positive="Yes")
evaluation("glm")
# Comparaison methode
evaluation <- function(approche){
mod <- train(Y~., data= donA, trControl=trainControl(method = "cv", number = 5), method=approche)
pred <- predict(mod, donT)
mat <- confusionMatrix(donT$Y, pred, positive="Yes")
return(c(max(mod$results[,"Accuracy"]),pred$results[,"Accuracy"]))
}
evaluation("glm")
evaluation <- function(approche){
mod <- train(Y~., data= donA, trControl=trainControl(method = "cv", number = 5), method=approche)
pred <- predict(mod, donT)
mat <- confusionMatrix(donT$Y, pred, positive="Yes")
return(c(max(mod$results[,"Accuracy"]),mat$overall[,"Accuracy"]))
}
evaluation("glm")
return(c(max(mod$results[,"Accuracy"]),mat$overall["Accuracy"]))
return(c(max(mod$results[,"Accuracy"]),mat$overall["Accuracy"]))
# Comparaison methode
evaluation <- function(approche){
mod <- train(Y~., data= donA, trControl=trainControl(method = "cv", number = 5), method=approche)
pred <- predict(mod, donT)
mat <- confusionMatrix(donT$Y, pred, positive="Yes")
return(c(max(mod$results[,"Accuracy"]),mat$overall["Accuracy"]))
}
evaluation("glm")
methodes <- c("glm","lda","svm")
evaluation(methodes)
sapply(methodes, evaluation)
methodes <- c("glm","lda","svmLinear")
perfos <- sapply(methodes, evaluation)
colnames(perfos) <- methodes
rownames(perfos) <- c("cv", "test set")
perfos
methodes <- c("glm","ridge","lasso", "enet", "rf", "rpart", "adaboost", "LogitBoost", "lda", "svmLinear")
perfos <- sapply(methodes, evaluation)
methodes <- c("glm","ridge","lasso", "enet", "rf", "rpart", "adaboost", "LogitBoost", "lda", "svmLinear")
perfos <- sapply(methodes, evaluation)
methodes <- c("glm","ridge","lasso", "enet")
perfos <- sapply(methodes, evaluation)
methodes <- c("glm","rf", "rpart", "adaboost", "LogitBoost", "lda", "svmLinear")
perfos <- sapply(methodes, evaluation)
perfos
methodes <- c("glm","rpart","lda", "svmLinear")
perfos <- sapply(methodes, evaluation)
colnames(perfos) <- methodes
rownames(perfos) <- c("cv", "test set")
perfos
# traitement des classes déséquilibrées
donDown <- downSample(donA[,-89],donA$Y, yname = "Y")
donDown
table(donDown$Y)
# traitement des classes déséquilibrées
donDown <- downSample(donA[,-89],donA$Y)
table(donDown$Y)
# traitement des classes déséquilibrées
donDown <- downSample(donA[,-89],donA$Y)
table(donDown$Y)
# traitement des classes déséquilibrées
donDown <- downSample(donA[,-89],donA$Y, yname = "Y")
table(donDown$Y)
table(donA)
table(donA$Y)
mod <- train(Y~., data=donDown, trControl=trainControl(method="none"), method="glm")
res[,"logDown"] <- predict(mod, donT)
res <- data.frame("Y"=donT$Y, "log"=0, "lift"=0, "svm"=0, "logDown")
res[,"logDown"] <- predict(mod, donT)
mat <- confusionMatrix(res[,"Y"],res[,"logDown"])
mat
mod <- train(Y~.,donA ,method = "glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type = "raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
# Methode d'apprentissage sans CV avec glm
fitControl<- trainControl(method = "none")
mod <- train(Y~.,donA ,method = "glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type = "raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
mat
donUp<- downSample(donA[,-89],donA$Y, yname = "Y")
table(donUp$Y)
table(donA$Y)
head(donUp)
table(donUp$Y)
table(donA$Y)
# traitement des classes déséquilibrées
donUp<- upSample(donA[,-89],donA$Y, yname = "Y")
table(donUp$Y)
mod <- train(Y~., data=donUp, trControl=trainControl(method="none"), method="glm")
res[,"logDown"] <- predict(mod, donT)
mat <- confusionMatrix(res[,"Y"],res[,"logDown"])
mat
