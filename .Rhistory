donDown <- downSample(donA[,-89],donA$Y)
table(donDown$Y)
# traitement des classes déséquilibrées
donDown <- downSample(donA[,-89],donA$Y, yname = "Y")
table(donDown$Y)
table(donA)
table(donA$Y)
mod <- train(Y~., data=donDown, trControl=trainControl(method="none"), method="glm")
res[,"logDown"] <- predict(mod, donT)
res <- data.frame("Y"=donT$Y, "log"=0, "lift"=0, "svm"=0, "logDown")
res[,"logDown"] <- predict(mod, donT)
mat <- confusionMatrix(res[,"Y"],res[,"logDown"])
mat
mod <- train(Y~.,donA ,method = "glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type = "raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
# Methode d'apprentissage sans CV avec glm
fitControl<- trainControl(method = "none")
mod <- train(Y~.,donA ,method = "glm", trControl=fitControl)
res[,"log"] <- predict(mod, donT,type = "raw")
mat <- confusionMatrix(res[,"Y"], res[,"log"])
mat
donUp<- downSample(donA[,-89],donA$Y, yname = "Y")
table(donUp$Y)
table(donA$Y)
head(donUp)
table(donUp$Y)
table(donA$Y)
# traitement des classes déséquilibrées
donUp<- upSample(donA[,-89],donA$Y, yname = "Y")
table(donUp$Y)
mod <- train(Y~., data=donUp, trControl=trainControl(method="none"), method="glm")
res[,"logDown"] <- predict(mod, donT)
mat <- confusionMatrix(res[,"Y"],res[,"logDown"])
mat
install.packages("blogdown")
library("rvest") #utile pour faire du webscrapping
library(stringr) #utile pour utiliser la fonction str_sub
table <- c() #définier un vecteur vide pour stocker les noms des tables
signification <- data.frame(code="",libel="") # définir un dataframe pour stocker variable et desc
# les différents topic de Nhanes
topic <- c("Demographics&CycleBeginYear=2015",
"Dietary&CycleBeginYear=2015",
"Examination&CycleBeginYear=2015",
"Laboratory&CycleBeginYear=2015",
"Questionnaire&CycleBeginYear=2015")
# Webscrapping de tous les noms de tables
for (i in 1:length(topic)){
url_nhanes <- "https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component="
url_topic <- topic[i]
url <- paste0(url_nhanes,url_topic)
data_html <- read_html(url)
questionnaire <- as.data.frame(data_html %>%
html_nodes(xpath = '(//*[@id="GridView1"])') %>%
html_table())
table <- c(table,str_sub(questionnaire$Doc.File,1,-5)) # avec stringr
}
table
don <- read.csv("data/var_lib.csv"")
don <- read.csv("data/var_lib.csv")
don <- read.csv2("data/var_lib.csv")
don <- read.csv2("nhanes/data/var_lib.csv")
don <- read.csv2("nhanesv2/data/var_lib.csv")
read.csv2("data/var_lib.csv")
read.csv2("data/var_lib.csv")
don <- read.csv2("data/var_lib.csv")
read.csv2("data/var_lib.csv")
read.csv2("data/var_lib.csv")
don <- read.csv2("var_lib.csv")
don <- read.csv2("data/var_lib.csv")
don <- read.csv2("data/var_lib.csv")
don <- read.csv2("data/var_lib.csv")
don <- read.csv2("data/var_lib.csv")
read.csv2("data/var_lib.csv")
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
head(don,15)
don$x <- NULL
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
head(don,15)
don
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
print(don[15,])
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
print(don[1:15,])
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
print(don[1:15,])
summary(don)
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
print(don[1:15,])
dim(don)
don <- read.csv2("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
print(don[1:15,])
don <- read.csv("F:/nhanesv2/data/var_lib.csv")
don <- read.csv("F:/nhanesv2/data/var_lib.csv")
don$x <- NULL
print(don[1:15,])
don <- read.csv("F:/nhanesv2/data/var_lib.csv")
don$X <- NULL
print(don[1:15,])
dim(don)
don <- read.csv("F:/nhanesv2/data/var_lib.csv")
don$X <- NULL
print(don[1:15,])
dim(don)
dim(don)
############################################################################
# Etude sur la prediction hypertension
############################################################################
library(glmnet)#contrainte
library(randomForest)#Foret
library(gbm)#Boosting
library(e1071)#SVM
library(rpart)#Arbre
library(foreach)#parallel
don <- read.csv("data/nhanes_hyper_mice.csv")
read.csv("data/nhanes_hyper_mice.csv")
############################################################################
# Etude sur la prediction hypertension
############################################################################
library(glmnet)#contrainte
library(randomForest)#Foret
library(gbm)#Boosting
library(e1071)#SVM
library(rpart)#Arbre
library(foreach)#parallel
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
levels(don$Y) <- c(0,1)
XX <- as.matrix(model.matrix(~.,don)[,-ncol(model.matrix(~.,don))])
YY <- as.matrix(model.matrix(~.,don)[,ncol(model.matrix(~.,don))])
bloc <- 4
ind= sample(1:nrow(don) %% 4+1)
res <- data.frame(Y=don$Y, log=0, ridge=0, lasso=0, elastic=0,
foret=0, adabo=0, logibo=0, svm=0, pm=0, arbre=0)
set.seed(1234)
deb <-   Sys.time()
foreach (i = 1:bloc, .packages = c("gbm","e1071","glmnet","randomForest", "rpart")) %dopar% {
# logisque
mod <- glm(Y~.,data=don[ind!=i,],family="binomial")
res[ind==i,"log"] <- predict(mod,don[ind==i,],type="response")
# XXA <- XX[ind!=i,]
# YYA <- YY[ind!=i,]
# XXT <- XX[ind==i,]
# # ridge
# tmp <- cv.glmnet(XXA,YYA,alpha=0,family="binomial")
# mod <- glmnet(XXA,YYA,alpha=0,lambda=tmp$lambda.min, family="binomial")
# res[ind==i,"ridge"] <- predict(mod,newx=XXT,type="response")
# # lass0
# tmp <- cv.glmnet(XXA,YYA, alpha=1, family="binomial")
# mod <- glmnet(XXA,YYA,alpha=1, lambda =tmp$lambda.1se,family="binomial" )
# res[ind==i,"lasso"] <- predict(mod,newx=XXT, type="response")
# # elasticnet
# tmp <- cv.glmnet(XXA,YYA, alpha=0.5, family="binomial")
# mod <- glmnet(XXA,YYA,alpha = 0.5, lambda = tmp$lambda.min, family="binomial")
# res[ind==i,"elastic"] <- predict(mod,newx=XXT,type="response")
# # foret
# mod <- randomForest(Y~., data = don[ind!=i,], ntree=500)
# res[ind==i, "foret"] <- predict(mod, don[ind==i,], type="prob")[,2]
# # Adaboost
# tmp <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = 500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = M)
# res[ind==i, "adabo"] <- predict(mod, newdata=don[ind==i,], type = "response", n.trees = M)
# # Logiboost
# tmp <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=M)
# res[ind==i, "logibo"] <- predict(mod,newdata=don[ind==i,], type= "response", n.trees = M)
# # SVM
# mod <- svm(Y~.,data=don[ind!=i,], kernel="linear",probability=T)
# # tmp <- tune(svm,Y~.,data=don[ind!=i,], kernel="linear",probability=T,ranges =
# #        list(cost=c(0.1,1,10,100)))
# # mod <- tmp$best.model
# res[ind==i,"svm"] <- attr(predict(mod,newdata = don[ind==i,],probability = T),"probabilities")[,2]
# # arbre
# mod <- rpart(Y~., data = don[ind!=i,], method="class")
# res[ind==i, "arbre"] <- predict(mod, don[ind==i,], type="prob")[,2]
}
fin <- Sys.time()
fin-deb
############################################
# Perceptron multicouche
############################################
library(keras)
install.packages("keras")
############################################################################
# Etude sur la prediction hypertension
############################################################################
library(glmnet)#contrainte
library(randomForest)#Foret
library(gbm)#Boosting
library(e1071)#SVM
library(rpart)#Arbre
library(foreach)#parallel
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
levels(don$Y) <- c(0,1)
XX <- as.matrix(model.matrix(~.,don)[,-ncol(model.matrix(~.,don))])
YY <- as.matrix(model.matrix(~.,don)[,ncol(model.matrix(~.,don))])
bloc <- 4
ind= sample(1:nrow(don) %% 4+1)
res <- data.frame(Y=don$Y, log=0, ridge=0, lasso=0, elastic=0,
foret=0, adabo=0, logibo=0, svm=0, pm=0, arbre=0)
set.seed(1234)
deb <-   Sys.time()
foreach (i = 1:bloc, .packages = c("gbm","e1071","glmnet","randomForest", "rpart")) %dopar% {
# logisque
mod <- glm(Y~.,data=don[ind!=i,],family="binomial")
res[ind==i,"log"] <- predict(mod,don[ind==i,],type="response")
# XXA <- XX[ind!=i,]
# YYA <- YY[ind!=i,]
# XXT <- XX[ind==i,]
# # ridge
# tmp <- cv.glmnet(XXA,YYA,alpha=0,family="binomial")
# mod <- glmnet(XXA,YYA,alpha=0,lambda=tmp$lambda.min, family="binomial")
# res[ind==i,"ridge"] <- predict(mod,newx=XXT,type="response")
# # lass0
# tmp <- cv.glmnet(XXA,YYA, alpha=1, family="binomial")
# mod <- glmnet(XXA,YYA,alpha=1, lambda =tmp$lambda.1se,family="binomial" )
# res[ind==i,"lasso"] <- predict(mod,newx=XXT, type="response")
# # elasticnet
# tmp <- cv.glmnet(XXA,YYA, alpha=0.5, family="binomial")
# mod <- glmnet(XXA,YYA,alpha = 0.5, lambda = tmp$lambda.min, family="binomial")
# res[ind==i,"elastic"] <- predict(mod,newx=XXT,type="response")
# # foret
# mod <- randomForest(Y~., data = don[ind!=i,], ntree=500)
# res[ind==i, "foret"] <- predict(mod, don[ind==i,], type="prob")[,2]
# # Adaboost
# tmp <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = 500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = M)
# res[ind==i, "adabo"] <- predict(mod, newdata=don[ind==i,], type = "response", n.trees = M)
# # Logiboost
# tmp <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=M)
# res[ind==i, "logibo"] <- predict(mod,newdata=don[ind==i,], type= "response", n.trees = M)
# # SVM
# mod <- svm(Y~.,data=don[ind!=i,], kernel="linear",probability=T)
# # tmp <- tune(svm,Y~.,data=don[ind!=i,], kernel="linear",probability=T,ranges =
# #        list(cost=c(0.1,1,10,100)))
# # mod <- tmp$best.model
# res[ind==i,"svm"] <- attr(predict(mod,newdata = don[ind==i,],probability = T),"probabilities")[,2]
# # arbre
# mod <- rpart(Y~., data = don[ind!=i,], method="class")
# res[ind==i, "arbre"] <- predict(mod, don[ind==i,], type="prob")[,2]
}
fin <- Sys.time()
fin-deb
############################################
# Perceptron multicouche
############################################
library(keras)
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 2, input_shape=ncol(XXA), activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss="mean_squared_error",
optimizer="sgd",
metrics="mae"
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=80,
batch_size=8
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
############################################################################
# Etude sur la prediction hypertension
############################################################################
library(glmnet)#contrainte
library(randomForest)#Foret
library(gbm)#Boosting
library(e1071)#SVM
library(rpart)#Arbre
library(foreach)#parallel
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
levels(don$Y) <- c(0,1)
XX <- as.matrix(model.matrix(~.,don)[,-ncol(model.matrix(~.,don))])
YY <- as.matrix(model.matrix(~.,don)[,ncol(model.matrix(~.,don))])
bloc <- 4
ind= sample(1:nrow(don) %% 4+1)
res <- data.frame(Y=don$Y, log=0, ridge=0, lasso=0, elastic=0,
foret=0, adabo=0, logibo=0, svm=0, pm=0, arbre=0)
set.seed(1234)
deb <-   Sys.time()
foreach (i = 1:bloc, .packages = c("gbm","e1071","glmnet","randomForest", "rpart")) %dopar% {
# logisque
mod <- glm(Y~.,data=don[ind!=i,],family="binomial")
res[ind==i,"log"] <- predict(mod,don[ind==i,],type="response")
XXA <- XX[ind!=i,]
YYA <- YY[ind!=i,]
XXT <- XX[ind==i,]
# ridge
tmp <- cv.glmnet(XXA,YYA,alpha=0,family="binomial")
mod <- glmnet(XXA,YYA,alpha=0,lambda=tmp$lambda.min, family="binomial")
res[ind==i,"ridge"] <- predict(mod,newx=XXT,type="response")
# lass0
tmp <- cv.glmnet(XXA,YYA, alpha=1, family="binomial")
mod <- glmnet(XXA,YYA,alpha=1, lambda =tmp$lambda.1se,family="binomial" )
res[ind==i,"lasso"] <- predict(mod,newx=XXT, type="response")
# elasticnet
tmp <- cv.glmnet(XXA,YYA, alpha=0.5, family="binomial")
mod <- glmnet(XXA,YYA,alpha = 0.5, lambda = tmp$lambda.min, family="binomial")
res[ind==i,"elastic"] <- predict(mod,newx=XXT,type="response")
# foret
mod <- randomForest(Y~., data = don[ind!=i,], ntree=500)
res[ind==i, "foret"] <- predict(mod, don[ind==i,], type="prob")[,2]
# Adaboost
tmp <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
shrinkage = 0.1,n.trees = 500)
M <- gbm.perf(tmp)[1]
mod <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
shrinkage = 0.1,n.trees = M)
res[ind==i, "adabo"] <- predict(mod, newdata=don[ind==i,], type = "response", n.trees = M)
# Logiboost
tmp <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
shrinkage=0.1,n.trees=500)
M <- gbm.perf(tmp)[1]
mod <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
shrinkage=0.1,n.trees=M)
res[ind==i, "logibo"] <- predict(mod,newdata=don[ind==i,], type= "response", n.trees = M)
# SVM
mod <- svm(Y~.,data=don[ind!=i,], kernel="linear",probability=T)
# tmp <- tune(svm,Y~.,data=don[ind!=i,], kernel="linear",probability=T,ranges =
#        list(cost=c(0.1,1,10,100)))
# mod <- tmp$best.model
res[ind==i,"svm"] <- attr(predict(mod,newdata = don[ind==i,],probability = T),"probabilities")[,2]
# arbre
mod <- rpart(Y~., data = don[ind!=i,], method="class")
res[ind==i, "arbre"] <- predict(mod, don[ind==i,], type="prob")[,2]
}
fin <- Sys.time()
fin-deb
############################################
# Perceptron multicouche
############################################
library(keras)
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 2, input_shape=ncol(XXA), activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss="mean_squared_error",
optimizer="sgd",
metrics="mae"
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=80,
batch_size=8
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
# matrice de confusion
monerreur(res[,2],res[,1])#log
monerreur <- function(X, Y, seuil=0.5){
table(cut(X, breaks = c(0,seuil,1)), Y)
}
# matrice de confusion
monerreur(res[,2],res[,1])#log
monerreur(res[,3],res[,1])#Ridge
monerreur(res[,4],res[,1])#Lasso
monerreur(res[,5],res[,1])#Elastic
monerreur(res[,6],res[,1])#Foret
monerreur(res[,7],res[,1])#Adabo
monerreur(res[,8],res[,1])#Logibo
monerreur(res[,9],res[,1])#SVM
monerreur(res[,10],res[,1])#perceptron
monerreur(res[,11],res[,1])#arbre
monerreurb <- function(X,Y,seuil=0.5){
Xc <- cut(X,breaks=c(0,seuil,1),labels=c(0,1))
sum(as.factor(Y)!=Xc)
}
apply(res[,-1],2,monerreurb,Y=res[,1],seuil=0.5)
library(pROC)
auc(res[,1],res[,2])#log
auc(res[,1],res[,3])#Ridge
auc(res[,1],res[,4])#Lasso
auc(res[,1],res[,5])#Elastic
auc(res[,1],res[,6])#Foret
auc(res[,1],res[,7])#Adabo
auc(res[,1],res[,8])#Logibo
auc(res[,1],res[,9])#SVM
auc(res[,1],res[,10])#perceptron
auc(res[,1],res[,11])#arbre
library(pROC)
auc(res[,1],res[,2])#log
auc(res[,1],res[,3])#Ridge
auc(res[,1],res[,4])#Lasso
auc(res[,1],res[,5])#Elastic
auc(res[,1],res[,6])#Foret
auc(res[,1],res[,7])#Adabo
auc(res[,1],res[,8])#Logibo
auc(res[,1],res[,9])#SVM
auc(res[,1],res[,10])#perceptron
auc(res[,1],res[,11])#arbre
plot(roc(res[,1],res[,2]))
lines(roc(res[,1],res[,3]), col="red")#log
lines(roc(res[,1],res[,4]), col="blue")#Ridge
lines(roc(res[,1],res[,5]), col="green")#Lasso
lines(roc(res[,1],res[,6]), col="brown")#Elastic
lines(roc(res[,1],res[,7]), col="yellow")#Adabo
lines(roc(res[,1],res[,8]), col="purple")#Logibo
lines(roc(res[,1],res[,9]), col="grey")#SVM
lines(roc(res[,1],res[,9]), col="black")#perceptron
lines(roc(res[,1],res[,9]), col="orange")#arbre
don <- read.csv("data/nhanes_hyper_mice.csv", row.names = 1)
don
summary(don)
sapply(colnames(don), function(x) class(don[[x]]))
colnames(don)
class(don[[Y]])
class(don[[colnames(don)]])
don[[Y]]
don[[VAR_TRAVAIL]]
don[["VAR_TRAVAIL"]]
don[,"VAR_TRAVAIL"]
sapply(colnames(don), function(x) class(don[,x]))
sapply(colnames(don), function(x) class(don[,x]))=="factor"
which(sapply(colnames(don), function(x) class(don[,x]))=="factor")
ind <- which(sapply(colnames(don), function(x) class(don[,x]))=="factor")
don[,ind]
don[,-ind]
library(ade4)
acm.disjonctif(don[,ind])
cbind(don[,-ind],acm.disjonctif(don[,ind]))
donb <- cbind(don[,-ind],acm.disjonctif(don[,ind]))
cah <- hclust(donb)
donb <- cbind(scale(don[,-ind]),acm.disjonctif(don[,ind]))
#ponderation par la frequence des indicatrices
PF <- function(x){
m <- mean(x)
return(x/sqrt(m))
}
don_disj <- sapply(acm.disjonctif(don[,ind],PF)
don_disj
#ponderation par la frequence des indicatrices
PF <- function(x){
m <- mean(x)
return(x/sqrt(m))
}
don_disj <- sapply(acm.disjonctif(don[,ind],PF)
don_disj
PF <- function(x){
m <- mean(x)
return(x/sqrt(m))
}
don_disj <- sapply(acm.disjonctif(don[,ind],PF)
don_disj <- sapply(acm.disjonctif(don[,ind],PF)
don_disj <- sapply(acm.disjonctif(don[,ind],PF))
acm.disjonctif(don[,ind])
don_disj <- sapply(acm.disjonctif(don[,ind]),PF)
donb <- cbind(scale(don[,-ind]),don_disj)
donb
don_disj
head(donb)
cah <- hclust(donb)
don <- read.csv("data/nhanes_hyper_mice.csv", row.names = 1)
ind <- which(sapply(colnames(don), function(x) class(don[,x]))=="factor")
don[,ind]
don[,-ind]
library(ade4)
#ponderation par la frequence des indicatrices
PF <- function(x){
m <- mean(x)
return(x/sqrt(m))
}
don_disj <- sapply(acm.disjonctif(don[,ind]),PF)
donb <- cbind(scale(don[,-ind]),don_disj)
head(donb)
cah <- hclust(donb)
cah <- hclust(dist(donb),method = "ward")
cah <- hclust(dist(donb),method = "ward.D2")
plot(as.dendrogram(cah))
rect.hclust(cah,h=140)
cutree(cah, h=3)
cutree(cah, h=140)
don[cutree(cah, h=140)==1,]
don[cutree(cah, h=140)==4,]
