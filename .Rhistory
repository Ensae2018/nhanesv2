layer_dense(units = 8, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=20,
batch_size=8
)
rm(pm.keras)
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 16, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=20,
batch_size=8
)
rm(pm.keras)
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=20,
batch_size=8
)
rm(pm.keras)
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=20,
batch_size=4
)
rm(pm.keras)
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=10,
batch_size=2
)
rm(pm.keras)
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=10,
batch_size=4
)
rm(pm.keras)
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=8,
batch_size=4
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
res
rm(pm.keras)
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=10,
batch_size=5
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
res
pm.keras %>% predict(XX[ind==i,])
write.csv2(res,"res_hyp.csv")
library(glmnet)#contrainte
library(randomForest)#Foret
library(gbm)#Boosting
library(e1071)#SVM
library(rpart)#Arbre
library(foreach)#parallel
library(glmnet)#contrainte
library(randomForest)#Foret
library(gbm)#Boosting
library(e1071)#SVM
library(rpart)#Arbre
library(foreach)#parallel
don <- read.csv("data/nhanes_hyper_mice.csv")
don$X <- NULL
levels(don$Y) <- c(0,1)
XX <- as.matrix(model.matrix(~.,don)[,-ncol(model.matrix(~.,don))])
YY <- as.matrix(model.matrix(~.,don)[,ncol(model.matrix(~.,don))])
bloc <- 4
ind= sample(1:nrow(don) %% 4+1)
res <- data.frame(Y=don$Y, log=0, logReduit=0, ridge=0, lasso=0, elastic=0,
foret=0, adabo=0, logibo=0, svm=0, pm=0, arbre=0)
set.seed(1234)
deb <-   Sys.time()
#foreach (i = 1:bloc, .packages = c("gbm","e1071","glmnet","randomForest", "rpart")) %dopar% {
for (i in 1:bloc){
# logisque
mod <- glm(Y~.,data=don[ind!=i,],family="binomial")
res[ind==i,"log"] <- predict(mod,don[ind==i,],type="response")
# logistique 9 variable
mod <- glm(Y~Age_in_years_at_screening+
Systolic_Blood_pres_2nd_rdg_mm_Hg+
high_cholesterol_level+
Body_Mass_Index_kg_m_2+
Doctor_ever_said_you_were_overweight+
Ever_told_doctor_had_trouble_sleeping+
Phosphorus_mg+
Diastolic_Blood_pres_1st_rdg_mm_Hg+
Sodium_mg,data=don[ind!=i,],family="binomial")
res[ind==i,"logReduit"] <- predict(mod,don[ind==i,],type="response")
XXA <- XX[ind!=i,]
YYA <- YY[ind!=i,]
XXT <- XX[ind==i,]
# ridge
tmp <- cv.glmnet(XXA,YYA,alpha=0,family="binomial")
mod <- glmnet(XXA,YYA,alpha=0,lambda=tmp$lambda.min, family="binomial")
res[ind==i,"ridge"] <- predict(mod,newx=XXT,type="response")
# lass0
tmp <- cv.glmnet(XXA,YYA, alpha=1, family="binomial")
mod <- glmnet(XXA,YYA,alpha=1, lambda =tmp$lambda.1se,family="binomial" )
#colnames(don[mod$beta@i])
res[ind==i,"lasso"] <- predict(mod,newx=XXT, type="response")
# elasticnet
tmp <- cv.glmnet(XXA,YYA, alpha=0.5, family="binomial")
mod <- glmnet(XXA,YYA,alpha = 0.5, lambda = tmp$lambda.min, family="binomial")
res[ind==i,"elastic"] <- predict(mod,newx=XXT,type="response")
# foret
mod <- randomForest(Y~., data = don[ind!=i,], ntree=500)
res[ind==i, "foret"] <- predict(mod, don[ind==i,], type="prob")[,2]
# Adaboost
tmp <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
shrinkage = 0.1,n.trees = 500)
M <- gbm.perf(tmp)[1]
mod <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
shrinkage = 0.1,n.trees = M)
res[ind==i, "adabo"] <- predict(mod, newdata=don[ind==i,], type = "response", n.trees = M)
# Logiboost
tmp <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
shrinkage=0.1,n.trees=500)
M <- gbm.perf(tmp)[1]
mod <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
shrinkage=0.1,n.trees=M)
res[ind==i, "logibo"] <- predict(mod,newdata=don[ind==i,], type= "response", n.trees = M)
# SVM
mod <- svm(Y~.,data=don[ind!=i,], kernel="linear",probability=T)
# tmp <- tune(svm,Y~.,data=don[ind!=i,], kernel="linear",probability=T,ranges =
#        list(cost=c(0.1,1,10,100)))
# mod <- tmp$best.model
res[ind==i,"svm"] <- attr(predict(mod,newdata = don[ind==i,],probability = T),"probabilities")[,2]
# arbre
mod <- rpart(Y~., data = don[ind!=i,], method="class")
res[ind==i, "arbre"] <- predict(mod, don[ind==i,], type="prob")[,2]
}
fin <- Sys.time()
fin-deb
res
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=10,
batch_size=5
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
write.csv2(res,"res_hyp.csv")
res[ind==i,"pm"]
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 6, input_shape=ncol(XXA), activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=10,
batch_size=5
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
pm.keras %>% predict(XX[ind==i,])
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 8, input_shape=ncol(XXA), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss="rmsprop",
optimizer="sgd",
metrics="accuracy"
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=15,
batch_size=8
)
i=1
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 8, input_shape=ncol(XXA), activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss="mean_squared_error",
optimizer="sgd",
metrics="mae"
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=30,
batch_size=8
)
pm.keras %>% predict(XX[ind==i,])
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 8, input_shape=ncol(XXA), activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss="mean_squared_error",
optimizer="sgd",
metrics="mae"
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=30,
batch_size=8
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
for (i in 1:bloc){
# instanciation du model
pm.keras <- keras_model_sequential()
# architecture
pm.keras %>%
layer_dense(units = 8, input_shape=ncol(XXA), activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
# configuration de l'apprentissage
pm.keras %>% compile(
loss="mean_squared_error",
optimizer="sgd",
metrics="mae"
)
# lancement de l'apprentissage
pm.keras %>% fit(
XXA <- XX[ind!=i,],
YYA <- YY[ind!=i,],
epochs=15,
batch_size=8
)
# proba prediction
res[ind==i,"pm"] <- pm.keras %>% predict(XX[ind==i,])
}
write.csv2(res,"res_hyp.csv")
write.csv2(res,"res_hyp.csv")
for (i in1:bloc){
# # logisque
# mod <- glm(Y~.,data=don[ind!=i,],family="binomial")
# res[ind==i,"log"] <- predict(mod,don[ind==i,],type="response")
# # logistique 9 variable
# mod <- glm(Y~Age_in_years_at_screening+
# Systolic_Blood_pres_2nd_rdg_mm_Hg+
# high_cholesterol_level+
# Body_Mass_Index_kg_m_2+
# Doctor_ever_said_you_were_overweight+
# Ever_told_doctor_had_trouble_sleeping+
# Phosphorus_mg+
# Diastolic_Blood_pres_1st_rdg_mm_Hg+
# Sodium_mg,data=don[ind!=i,],family="binomial")
# res[ind==i,"logReduit"] <- predict(mod,don[ind==i,],type="response")
# XXA <- XX[ind!=i,]
# YYA <- YY[ind!=i,]
# XXT <- XX[ind==i,]
# # ridge
# tmp <- cv.glmnet(XXA,YYA,alpha=0,family="binomial")
# mod <- glmnet(XXA,YYA,alpha=0,lambda=tmp$lambda.min, family="binomial")
# res[ind==i,"ridge"] <- predict(mod,newx=XXT,type="response")
# # lass0
# tmp <- cv.glmnet(XXA,YYA, alpha=1, family="binomial")
# mod <- glmnet(XXA,YYA,alpha=1, lambda =tmp$lambda.1se,family="binomial" )
# #colnames(don[mod$beta@i])
# res[ind==i,"lasso"] <- predict(mod,newx=XXT, type="response")
# # elasticnet
# tmp <- cv.glmnet(XXA,YYA, alpha=0.5, family="binomial")
# mod <- glmnet(XXA,YYA,alpha = 0.5, lambda = tmp$lambda.min, family="binomial")
# res[ind==i,"elastic"] <- predict(mod,newx=XXT,type="response")
# # foret
# mod <- randomForest(Y~., data = don[ind!=i,], ntree=500)
# res[ind==i, "foret"] <- predict(mod, don[ind==i,], type="prob")[,2]
# # Adaboost
# tmp <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = 500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = M)
# res[ind==i, "adabo"] <- predict(mod, newdata=don[ind==i,], type = "response", n.trees = M)
# # Logiboost
# tmp <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=M)
# res[ind==i, "logibo"] <- predict(mod,newdata=don[ind==i,], type= "response", n.trees = M)
# SVM
mod <- svm(Y~.,data=don[ind!=i,], kernel="linear",probability=T)
# tmp <- tune(svm,Y~.,data=don[ind!=i,], kernel="linear",probability=T,ranges =
#        list(cost=c(0.1,1,10,100)))
# mod <- tmp$best.model
res[ind==i,"svm"] <- attr(predict(mod,newdata = don[ind==i,],probability = T),"probabilities")[,2]
# # arbre
# mod <- rpart(Y~., data = don[ind!=i,], method="class")
# res[ind==i, "arbre"] <- predict(mod, don[ind==i,], type="prob")[,2]
}
for (i in 1:bloc){
# # logisque
# mod <- glm(Y~.,data=don[ind!=i,],family="binomial")
# res[ind==i,"log"] <- predict(mod,don[ind==i,],type="response")
# # logistique 9 variable
# mod <- glm(Y~Age_in_years_at_screening+
# Systolic_Blood_pres_2nd_rdg_mm_Hg+
# high_cholesterol_level+
# Body_Mass_Index_kg_m_2+
# Doctor_ever_said_you_were_overweight+
# Ever_told_doctor_had_trouble_sleeping+
# Phosphorus_mg+
# Diastolic_Blood_pres_1st_rdg_mm_Hg+
# Sodium_mg,data=don[ind!=i,],family="binomial")
# res[ind==i,"logReduit"] <- predict(mod,don[ind==i,],type="response")
# XXA <- XX[ind!=i,]
# YYA <- YY[ind!=i,]
# XXT <- XX[ind==i,]
# # ridge
# tmp <- cv.glmnet(XXA,YYA,alpha=0,family="binomial")
# mod <- glmnet(XXA,YYA,alpha=0,lambda=tmp$lambda.min, family="binomial")
# res[ind==i,"ridge"] <- predict(mod,newx=XXT,type="response")
# # lass0
# tmp <- cv.glmnet(XXA,YYA, alpha=1, family="binomial")
# mod <- glmnet(XXA,YYA,alpha=1, lambda =tmp$lambda.1se,family="binomial" )
# #colnames(don[mod$beta@i])
# res[ind==i,"lasso"] <- predict(mod,newx=XXT, type="response")
# # elasticnet
# tmp <- cv.glmnet(XXA,YYA, alpha=0.5, family="binomial")
# mod <- glmnet(XXA,YYA,alpha = 0.5, lambda = tmp$lambda.min, family="binomial")
# res[ind==i,"elastic"] <- predict(mod,newx=XXT,type="response")
# # foret
# mod <- randomForest(Y~., data = don[ind!=i,], ntree=500)
# res[ind==i, "foret"] <- predict(mod, don[ind==i,], type="prob")[,2]
# # Adaboost
# tmp <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = 500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data = don[ind!=i,], distribution = "adaboost", interaction.depth = 2,
#            shrinkage = 0.1,n.trees = M)
# res[ind==i, "adabo"] <- predict(mod, newdata=don[ind==i,], type = "response", n.trees = M)
# # Logiboost
# tmp <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=500)
# M <- gbm.perf(tmp)[1]
# mod <- gbm(as.numeric(Y)-1~.,data=don[ind!=i,], distribution="bernoulli", interaction.depth = 2,
#            shrinkage=0.1,n.trees=M)
# res[ind==i, "logibo"] <- predict(mod,newdata=don[ind==i,], type= "response", n.trees = M)
# SVM
mod <- svm(Y~.,data=don[ind!=i,], kernel="linear",probability=T)
# tmp <- tune(svm,Y~.,data=don[ind!=i,], kernel="linear",probability=T,ranges =
#        list(cost=c(0.1,1,10,100)))
# mod <- tmp$best.model
res[ind==i,"svm"] <- attr(predict(mod,newdata = don[ind==i,],probability = T),"probabilities")[,2]
# # arbre
# mod <- rpart(Y~., data = don[ind!=i,], method="class")
# res[ind==i, "arbre"] <- predict(mod, don[ind==i,], type="prob")[,2]
}
res
write.csv2(res,"res_hyp.csv",row.names = F)
